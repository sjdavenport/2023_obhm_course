{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDP Inference in Regression - OHBM 2023\n",
    "## Beyond Blobology: Advances in Statistical Inference for Neuroimaging\n",
    "\n",
    "In this practical you will learn how to perform TDP inference in general linear models. At the end of the practical you should be able to provide TDP bounds in the regression setting. We will begin with a simulation setting in which we shall introduce the main functions needed and introduce some of the basic concepts. We shall then apply these methods to a regression example from the HCP Working-Memory task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started we need to install and import the following python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/sjdavenport/pyperm-install\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as npm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sanssouci as ss\n",
    "\n",
    "import pyperm as pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details on the pyperm package, see https://github.com/sjdavenport/pyperm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimension of the example and the number of subjects\n",
    "Dim = (50,50)\n",
    "N = 100\n",
    "nvoxels = np.prod(Dim)\n",
    "\n",
    "# Generate the category vector and obtain the corresponding design matrix\n",
    "from sklearn.utils import check_random_state\n",
    "rng = check_random_state(101)\n",
    "categ = rng.choice(3, N, replace = True)\n",
    "X = pr.group_design(categ)\n",
    "print(X[0:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the contrast matrix (here 2 contrasts are chosen)\n",
    "C = np.array([[1, -1, 0], [0, 1, -1]])\n",
    "\n",
    "# Calulate the number contrasts\n",
    "L = C.shape[0]\n",
    "\n",
    "# Calculate the number of p-values generated (L for each voxel)\n",
    "m = nvoxels * L\n",
    "\n",
    "# Generate a stationary random field with given FWHM\n",
    "FWHM = 4; lat_data = pr.statnoise(Dim, N, FWHM)\n",
    "\n",
    "# Alternatively could generate a white noise field\n",
    "#lat_data = pr.wfield(Dim, N)\n",
    "\n",
    "# Plot a sample realization of the noise\n",
    "plt.imshow(lat_data.field[:, :, 1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add signal to the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the locations where the category is 2\n",
    "w2 = np.where(categ==2)[0]\n",
    "\n",
    "# Initialize the spatial signal\n",
    "pi0 = 0.9  # proportion of noise (true null hypotheses)\n",
    "p0 = int(np.round(pi0 * nvoxels))\n",
    "signal = np.zeros(nvoxels)\n",
    "signal[(p0 + 1): nvoxels] = 0.7\n",
    "signal = signal.reshape(Dim)\n",
    "\n",
    "# Add the signal to the field\n",
    "for I in np.arange(len(w2)):\n",
    "    lat_data.field[:, :, w2[I]] += signal\n",
    "\n",
    "# Convert the signal to boolean to determine whether the true signal is\n",
    "bool_signal = np.zeros(Dim + (L,)) == 0\n",
    "bool_signal[:, :, 1] = signal > 0 \n",
    "\n",
    "# Plot the locaion locations for illustration\n",
    "plt.imshow(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthoc Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is performed using the residuals of the linear model. This gives test-statistics that have the same asymptotic distribution as the limiting test-statistic (under the null). See Eck 2017 and Freedman 1981 for further details. In our context we use these to obtain bootstrapped pivotal statistics which allow us to obtain asymptotic JER control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of bootstraps to use\n",
    "B = 500\n",
    "\n",
    "# Choose the template to use (by default the linear template is chosen)\n",
    "template = 'linear'\n",
    "\n",
    "# Run the bootstrapped algorithm (takes a few seconds to run)\n",
    "minPperm, orig_pvalues, pivotal_stats, bs = pr.boot_contrasts(lat_data, X, C, B, template, True, 1, display_progress=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_sort_idx = np.argsort(np.ravel(orig_pvalues.field))\n",
    "pvals = np.ravel(orig_pvalues.field)[pval_sort_idx]\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=2) \n",
    "plt.subplot(121)\n",
    "plt.hist(np.ravel(orig_pvalues.field), 100)\n",
    "plt.title('Histogram of the p-values')\n",
    "plt.ylabel('Counts')\n",
    "\n",
    "npvals = len(pvals)\n",
    "plt.subplot(122)\n",
    "plt.plot(pvals[:np.min([1000, npvals])])\n",
    "plt.title('Smallest 1000 p-values')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('p_{(k)}')\n",
    "\n",
    "figure.tight_layout(pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The bootstrap distribution of the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minPperm is the vector of permuted minimum statistics, importantly it includes the original data as one of the permutations in order to avoid 0 p-values. In particular, for $b = 1, ..., B$, minPperm[b-1] = $\\min_{l,v} p^b_{l,v}$ where $p^b_{l,v}$ denotes the $b$th bootstrap of the p-value at voxel $v$ and contrast $l$. We visualize its distribution of the values in minPperm in the following plot. Note that the distribution is skewed to the left because it is the distribution of the minimum of many p-values under the global null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(minPperm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The bootstrap distribution of the pivotal statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously when seeking to provide simultaneous bounds on the TDP we instead must use quantiles of the pivotal statistics. To do so we can use the vector pivotal_stats. Given a given template family of invertible functions: $(t_k)_{1 \\leq k \\leq m}$ for $b = 1, ..., B$, we have pivotal_stats[b-1] = $\\min_{1 \\leq k \\leq m} t_k^{-1}(p_{(k:m)}^b)$ where $p_{(k:m)}^b$ is the $b$ th bootstrap of the $k$th minimum p-value. In what follows we shall use the linear template family, which is defined as $t_k(\\lambda) = \\frac{\\lambda k}{m}$.\n",
    "\n",
    "Earlier today we showed that, given $\\alpha \\in (0,1)$, taking $\\lambda_\\alpha^*$ to be the $\\alpha$-quantile of the distribution of the pivotal statistics enables us to provide valid simultaneous bounds on the true discovery proportion (TDP) given any subset of the p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pivotal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bootstrapped pivotal_stats that we have calculated we can choose a value lambda that is the $\\alpha\\%$ quantile of the distribution in order to provide asymptotically valid bounds on the TDP which hold with probability $1-\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the lambda quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the confidence level\n",
    "alpha = 0.1\n",
    "\n",
    "# Obtain the lambda calibration\n",
    "lambda_star_alpha = np.quantile(pivotal_stats, alpha)\n",
    "print('Lambda Quantile:', lambda_star_alpha)\n",
    "\n",
    "# Calculate the number of voxels in the mask\n",
    "nvoxels = np.sum(lat_data.mask)\n",
    "\n",
    "# Gives t_k(lambda_star_alpha) = lambda_star_alpha*k/npvals for k = 1, ..., npvals\n",
    "thr = ss.linear_template(lambda_star_alpha, npvals, npvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the code below, thr[k-1] = $\\frac{\\lambda^*_{\\alpha}k}{m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(thr)\n",
    "print(lambda_star_alpha*(1/m))\n",
    "print(lambda_star_alpha*(2/m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PostHoc Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a chosen subset of voxels we can use this alpha quantile to provide a bound on the number of true null hypotheses within that subset. Try varying the set of p-values used to see how the bound changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 75 pvalues (or any subset of the p-values)\n",
    "npvals = 75\n",
    "subset_pvals = np.sort(np.ravel(orig_pvalues.field))[:npvals]\n",
    "\n",
    "# Compute an upper bound on the number of null hypotheses\n",
    "bound = ss.max_fp(subset_pvals, thr)\n",
    "print('FP upper Bound on subset:', bound)\n",
    "print('TP lower Bound on subset:', npvals - bound)\n",
    "print('FDP upper Bound on subset:', bound/npvals)\n",
    "print('TP lower Bound on subset:', (npvals - bound)/npvals)\n",
    "\n",
    "confidence = 1-alpha\n",
    "print('\\nWe can thus conclude that with probability', confidence, 'that', npvals - bound, 'of the hypotheses within the given subset are active.')\n",
    "print('As such we obtain a TDP lower bound of', np.round((npvals - bound)/npvals,2), 'on the proportion of active hypothesis with that subset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can vary the number 75 to see the difference in the bounds provided. Try also choosing different subsets of p-values to see what bounds you get. If you choose the pvalues where there is signal what bounds do you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to calculate confidence envelopes. I.e. for i = 1:npvals, max_FP[i-1] is the upper bound on the number of false positives that occur within the set [p[0], \\dots, p[i-1]] if you were to reject all elements of that set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_FP = ss.curve_max_fp(subset_pvals, thr) # Confidence envelope on the chosen subset\n",
    "print(max_FP)\n",
    "max_FP = ss.curve_max_fp(pvals, thr) # Confidence envelope on all of them\n",
    "print(max_FP[0: 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the FPR and TP curve bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below show the bounds we can provide within the minimum $k$ pvalues, simultaneously over $k$. Try increasing the variable npvals above to zoom out and see a bit more. What happens to the bounds provided as you increase npvals? What is the cause behind this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of p-values (always the smallest ones first) to plot\n",
    "npvals = 75\n",
    "subset_pvals = np.sort(np.ravel(orig_pvalues.field))[:npvals]\n",
    "lowestnumber = npvals\n",
    "\n",
    "# Generate the vector [0,...,npvals]\n",
    "one2npvals = np.arange(1, npvals + 1)\n",
    "\n",
    "# Ensure that selected number is not greater than the total number of p-values\n",
    "lowestnumber = np.min([lowestnumber, npvals])\n",
    "\n",
    "# Dividing the envelope by the number of elements in the set gives a bound on the false discovery proportion\n",
    "max_FDP = max_FP[0: lowestnumber] / one2npvals[0: lowestnumber] \n",
    "min_TP = one2npvals[0: lowestnumber] - max_FP[0: lowestnumber]\n",
    "\n",
    "# Calculate the truth (to determine if it is correctly bounded!)\n",
    "sorted_signal = np.ravel(bool_signal)[pval_sort_idx]\n",
    "TP = np.zeros(lowestnumber)\n",
    "for I in np.arange(lowestnumber):\n",
    "    TP[I] = np.sum(sorted_signal[0: I + 1])\n",
    "    \n",
    "# Calculate the true FDP for each subset\n",
    "FP = np.zeros(lowestnumber)\n",
    "for I in np.arange(lowestnumber):\n",
    "    FP[I] = np.sum(abs(sorted_signal[0: I + 1] - 1))\n",
    "true_FDP = FP / one2npvals[0: lowestnumber] \n",
    "    \n",
    "# Initialize the figure\n",
    "figure = plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot the false discovery proportion and its bound\n",
    "plt.subplot(121)\n",
    "plt.plot(max_FDP, label='FDP bound')\n",
    "plt.plot(true_FDP, label='True FDP')\n",
    "plt.title('Upper bound on FDP amongst smallest p-values')\n",
    "plt.xlim(1, lowestnumber)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('FDP(p_{(1)}, \\dots, p_{(k)}')\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Plot the true postives and their bound\n",
    "plt.subplot(122)\n",
    "plt.plot(min_TP, label='TP bound')\n",
    "plt.plot(TP, label='True Positives')\n",
    "plt.title('Lower bound on TP')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlim(1, lowestnumber)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('TP(p_{(1)}, \\dots, p_{(k)}')\n",
    "#figure, axes = plt.subplots(nrows=1, ncols=2) \n",
    "figure.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxbootpval = 0\n",
    "kmax = 100\n",
    "kvals = np.arange(kmax)\n",
    "for b in np.arange(B):\n",
    "    sorted_bootstraps = np.sort(bs[b,:])\n",
    "    plt.plot(sorted_bootstraps, color=\"blue\")\n",
    "    maxbootpval = np.max((maxbootpval, np.max(sorted_bootstraps[:kmax])))\n",
    "    \n",
    "plt.plot(thr,color=\"black\", linewidth = 3)\n",
    "    \n",
    "plt.xlim(0, kmax)\n",
    "plt.ylim(0, maxbootpval)\n",
    "#plt.ylim(0, 0.0005)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('p_{b,(i)}')\n",
    "plt.title('Plotting the ordered p-values for each bootstrap')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand what this plot is showing. What do each of the blue lines represent? What is special about the black curve displayed? Try varying kmax to. Try taking kmax to be small in order to zoom in to the bottom left corner of the plot. Take kmax to be large to see how the plot changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back through the above data example. Try varying parameters like the level of smoothness, $\\pi_0$ - the number of null hypotheses, the magnitude of the signal and the shape and size of the signal. Examine the effect of the number of bootstraps (say changing B to 1000), does this have an effect on the bounds provided? Does this effect depend on the magnitude of the signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D fMRI Real Data Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** To participate in this section, you **MUST** sign the 'Data Use Terms' (if you haven't already done so). Please follow the instructions given in [HCP_Access_Form_Instructions.pdf](./HCP_Access_Form_Instructions.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data directory\n",
    "import os\n",
    "from data.download import download_and_extract_zip_from_dropbox\n",
    "data_dir = os.path.join(os.getcwd(),'data')\n",
    "real_data_dir = os.path.join(os.getcwd(),'data','example_real_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data - this takes a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide Dropbox file URL and Local system path where file needs to be downloaded and extracted.\n",
    "bold_files, covariates = download_and_extract_zip_from_dropbox(real_data_dir)\n",
    "data_loc = real_data_dir + '\\\\HCP_U77_WM\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively if you have already downloaded this data during one of the previous practicals you don't need to run the code in the box above. If so make sure that the data is saved in the real_data_dir directory - if not you may need to change that variable to the location where the data is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to run if you already had the data downloaded:\n",
    "data_loc = real_data_dir + '\\\\HCP_U77_WM\\\\'\n",
    "bold_files = pr.list_files(data_loc, '.nii.gz', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was derived from the Wu-Minn HCP working memory task. Here is some further detail on the task itself.\n",
    "\n",
    "**The Task**\n",
    "\n",
    "In the experiment, 77 unrelated subjects performed two tasks spread across two runs where each run contained four blocks. During each block, the subject undertook either a 2-back memory task or a 0-back control task. The experimental design was arranged such that, in each run, two blocks were designated to the 2-back memory task, and two blocks were designated to the 0-back control task. In each block a subject was shown a stimuli image (a picture of a face or a place, for instance) and then asked to recall the image they were shown. They were either asked to recall the most recent image (the '0-back' image) or the image shown to them two images prior (the '2-back' image). Interest lies in assessing whether this delay impacted the \\%BOLD response.\n",
    "\n",
    "**First-level Analysis**\n",
    "\n",
    "In FSL, a first-level analysis has been conducted independently for each subject. In each first-level analysis, the task design was regressed onto Blood Oxygenation Level Dependent (BOLD) response and a Contrast Parameter Estimate (COPE) map was generated. Each COPE map represents, for a given subject, the difference in BOLD response between the subject performing the 2-back task and the 0-back task (e.g $L\\hat{\\beta}=\\hat{\\beta}_{\\text{2-Back}}-\\hat{\\beta}_{\\text{0-Back}})$).\n",
    "\n",
    "**Acquisition Details**\n",
    "\n",
    "All image acquisitions were obtained using a $32$ channel head coil on a modified $3T$ Siemans Skyra scanner via a gradient-echo EPI sequence with TR $=720$ ms, TE $=33.1$ ms, $2.0$ mm slice thickness, $72$ slices, $2.0$ mm isotropic voxels, $208$ mm $\\times 180$ mm FOV, and a multi-band acceleration factor of $8$.\n",
    "\n",
    "**The Data**\n",
    "\n",
    "You have the COPE map for each subject and the following covariates for each subject, both sorted by subject ID.\n",
    " - `Age`: The subject's age.\n",
    " - `Sex`: The subject's biological sex, with male encoded as `0` and female encoded as `1`.\n",
    " - `PMAT24_A_CR`: The number of correct responses from the Penn Progressive matrices; a measure of the subject's fluid intelligence.\n",
    " \n",
    "For more information on these variables, please visit [this link](https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release). Here is a brief view of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Task**\n",
    "\n",
    "The task is to fit the following model at each voxel $v$ and to provide TDP bounds on selected regions. \n",
    "\n",
    "$$Y(v) = \\beta_0 + \\text{Sex} \\beta_1 + \\text{Age} \\beta_2 + \\text{PMAT24_A_CR} \\beta_3 $$\n",
    "\n",
    "To do so you need to load in the data and run the methods above to obtain a lambda calibration. You can then select subset of the data of your choosing and provide bounds on the number of true postives in each subset.\n",
    "\n",
    "Feel free to look at the data first. The bounds are valid for any subset, including data-driven ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the data into memory and apply some smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import get_data, load_img\n",
    "from nilearn.input_data import NiftiMasker\n",
    "fwhm = 5 # Set the smoothness parameter (in mm)\n",
    "mask = data_dir + '\\\\mask_GM_forFunc.nii'\n",
    "masker = NiftiMasker(smoothing_fwhm=fwhm, mask_img=mask).fit()\n",
    "data = masker.transform(bold_files).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the mean of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = np.mean(data, 1)\n",
    "data_mean_3d = np.squeeze(\n",
    "    get_data(masker.inverse_transform(data_mean.transpose())))\n",
    "plt.imshow(data_mean_3d[:,:,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.ravel(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will load in the covariates. The covariates file: 'behavioural_data_subset_77.csv' corresponds to a subset of the unresticted covariates of the 77 subjects. The full set of unrestricted is available on the HCP website MAKE LINK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_data = pd.read_csv(\n",
    "    data_loc + 'behavioural_data_subset_77.csv')\n",
    "print(pd_data)\n",
    "design = np.zeros((77, 4))\n",
    "design[:, 1:4] = pd_data.values[:, 1:4]\n",
    "\n",
    "# Add an intercept:\n",
    "design[:, 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try exploring the above covariates to get a better idea of their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field = pr.make_field(data.copy())\n",
    "\n",
    "# Obtain the number of subjects\n",
    "nsubj = data_field.fibersize\n",
    "\n",
    "contrast_matrix = np.array((0,0,0,1))\n",
    "n_params = 4\n",
    "\n",
    "# Obtain the test statistics and convert to p-values\n",
    "test_stats, _ = pr.contrast_tstats(data_field, design, contrast_matrix)\n",
    "\n",
    "# pvalues = 2*(1 - t.cdf(abs(test_stats.field), nsubj-n_params))\n",
    "pvalues = pr.tstat2pval(test_stats.field, nsubj-n_params, 1)\n",
    "\n",
    "tstat_3d = np.squeeze(\n",
    "    get_data(masker.inverse_transform((test_stats.field).transpose())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tstat_3d[:,:,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 100\n",
    "minPperm, origpvals, pivotal_stats, bootstore = pr.boot_contrasts(\n",
    "    data.copy(), design, contrast_matrix, n_bootstraps=n_bootstraps,\n",
    "    display_progress=1, store_boots=1, doonesample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "lambda_quant = np.quantile(pivotal_stats, alpha)\n",
    "nhypotheses = np.prod(origpvals.field.shape)\n",
    "\n",
    "sorted_pvalues = np.sort(origpvals.field, 0)\n",
    "pvalue_subset = sorted_pvalues[0:700] # Chose a subset of the smallest p-values\n",
    "\n",
    "FP_bound, TP_bound, FDP_bound, TDP_bound = pr.get_bounds(\n",
    "    pvalue_subset, lambda_quant, nhypotheses)\n",
    "\n",
    "confidence = 1-alpha\n",
    "print('\\nWe can thus conclude that with probability', confidence, 'that', TP_bound, 'of the hypotheses within the given subset are active.')\n",
    "print('As such we obtain a TDP lower bound of', np.round(TDP_bound,2), 'on the proportion of active hypothesis with that subset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now exmaine some of the superthreshold clusters. To do so we choose a cluster defining threshold, 2 and a minimum clustersize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_im = load_img(mask).get_fdata()\n",
    "cluster_defining_threshold = 2\n",
    "min_cluster_size = 500\n",
    "cluster_im, cluster_sizes = pr.find_clusters(tstat_3d, cluster_defining_threshold, below=0, mask=mask_im, min_cluster_size = min_cluster_size)\n",
    "print(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the TP bounds within the clusters\n",
    "pvalues_3d = np.squeeze(\n",
    "    get_data(masker.inverse_transform(pvalues.transpose())))\n",
    "\n",
    "cluster_number = 1 # Can change this to be 1, 2 or 3\n",
    "region_idx = cluster_im == cluster_number\n",
    "FP_bound, TP_bound, FDP_bound, TDP_bound = pr.get_bounds(\n",
    "    pvalues_3d[region_idx], lambda_quant, nhypotheses)\n",
    "confidence = 1-alpha\n",
    "print('\\nWe can thus conclude that with probability', confidence, 'that', TP_bound, 'of the hypotheses within the chosen cluster are active.')\n",
    "print('As such we obtain a TDP lower bound of', np.round(TDP_bound,2), 'on the proportion of active hypothesis with that cluster.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus conclude that with 80% = 100(1-$\\alpha$)% probability the largest cluster contains a number of active voxels. Instead for for the second largest cluster (set cluster_number = 2 to see this) we are only able to say that there is at least 1 active voxel and for the third largest (cluster_number = 2) we cannot provide an interesting lower bound. This gives us a greater insight into the contents of each cluster, something that clustersize inference would not be able to do. Try varying the cluster defining threshold to see how the conclusions change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the largest cluster below for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "region_idx_plot = 0.5*region_idx\n",
    "region_idx_nifti = masker.inverse_transform(region_idx_plot[mask_im>0])\n",
    "plotting.plot_stat_map(\n",
    "        region_idx_nifti,\n",
    "        display_mode='z', vmax=2, colorbar = False,\n",
    "        title='Slices through the cluster', cut_coords=[40, 50, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if the above code doesn't run, it may be because you need to upgrade matplotlib. To do so run the following code and then try the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflecting on the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you decrease the value of alpha, the bounds on the number of active voxels decrease. In order to increase the signal to noise ratio, either additional smoothing can be applied. Alternatively a larger dataset can be used. The HCP dataset consists of 1200 subjects in total so if they could all be used then it would be easier to find an interesting effect. Here we only use a subset of 77 for computational ease in the practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you plot a confidence curve plot for the TDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of other contrasts that could be of interest here? Experiment with changing the contrast_matrix. How about other models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also test how varying the smoothness affects the results. Increasing the amount of smoothing increases the signal to noise ratio and will give you more power at the price of spatial specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increasing the number of bootstraps, does this make any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the simultaneity gaurantees assume a fixed level of smoothing. In order to provide simultaneous guarantees over multiple smoothing levels the joint distribution pvalues over the different smoothing levels would have to be bootstrapped. This is possible but is not implemented here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
